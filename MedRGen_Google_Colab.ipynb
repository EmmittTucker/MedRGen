{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Medical Reasoning Dataset Generator (MedRGen) - Google Colab\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/your-username/MedRGen/blob/main/MedRGen_Google_Colab.ipynb)\n",
        "\n",
        "## üéØ Project Overview\n",
        "\n",
        "MedRGen is a professional synthetic medical reasoning dataset generator that creates high-quality medical cases through simulated doctor-patient interactions. This notebook provides a complete setup and usage guide for running MedRGen on Google Colab.\n",
        "\n",
        "### Key Features:\n",
        "- üè• **Multi-Specialty Support**: Internal medicine, cardiology, emergency medicine, family medicine\n",
        "- üë• **Realistic Demographics**: Diverse patient profiles with appropriate medical histories  \n",
        "- üí¨ **Natural Conversations**: Multi-turn doctor-patient dialogues following clinical patterns\n",
        "- üß† **Evidence-Based Reasoning**: Medical expert reasoning using current clinical guidelines\n",
        "- üìä **Quality Assurance**: Dual dataset generation with LLM evaluation\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö†Ô∏è Important Requirements\n",
        "\n",
        "Before running this notebook, you'll need:\n",
        "1. **OpenAI API Key** with access to GPT-4 and O1-preview models\n",
        "2. **Google Colab Pro** (recommended) for better performance and longer runtimes\n",
        "3. **External Storage** (Google Drive) for storing generated datasets\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Step 1: Environment Setup and Installation\n",
        "\n",
        "Let's start by setting up the environment and installing all required dependencies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check Python version and system info\n",
        "import sys\n",
        "import platform\n",
        "print(f\"Python version: {sys.version}\")\n",
        "print(f\"Platform: {platform.platform()}\")\n",
        "print(f\"Architecture: {platform.architecture()}\")\n",
        "\n",
        "# Check if we're running on Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    print(\"‚úÖ Running on Google Colab\")\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    print(\"‚ùå Not running on Google Colab\")\n",
        "    IN_COLAB = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive for external storage (recommended for large datasets)\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    \n",
        "    # Create project directory in Google Drive\n",
        "    import os\n",
        "    project_path = \"/content/drive/MyDrive/MedRGen_Projects\"\n",
        "    os.makedirs(project_path, exist_ok=True)\n",
        "    print(f\"‚úÖ Project directory created: {project_path}\")\n",
        "else:\n",
        "    project_path = \"/content/MedRGen_Projects\"\n",
        "    os.makedirs(project_path, exist_ok=True)\n",
        "    print(f\"‚úÖ Project directory created: {project_path}\")\n",
        "\n",
        "# Change to project directory\n",
        "os.chdir(project_path)\n",
        "print(f\"üìÅ Current working directory: {os.getcwd()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the MedRGen repository\n",
        "!git clone https://github.com/your-username/MedRGen.git\n",
        "os.chdir(\"MedRGen\")\n",
        "print(f\"üìÅ Changed to MedRGen directory: {os.getcwd()}\")\n",
        "\n",
        "# List directory contents to verify clone\n",
        "print(\"\\nüìÇ Repository contents:\")\n",
        "!ls -la\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required dependencies\n",
        "print(\"üì¶ Installing dependencies...\")\n",
        "%pip install openai>=1.0.0 pydantic>=2.0.0 PyYAML>=6.0.0 pandas>=2.0.0 numpy>=1.24.0 tqdm>=4.65.0 python-dotenv>=1.0.0 jsonschema>=4.17.0 pytest>=7.0.0 pytest-cov>=4.0.0 typing-extensions>=4.5.0 requests>=2.28.0 click>=8.0.0 rich>=13.0.0\n",
        "\n",
        "print(\"\\n‚úÖ Dependencies installed successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîë Step 2: API Key Configuration\n",
        "\n",
        "**Important**: You'll need an OpenAI API key with access to GPT-4 and O1-preview models. Never share your API keys publicly!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure OpenAI API Key\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# Option 1: Secure input (recommended for Colab)\n",
        "if IN_COLAB:\n",
        "    openai_api_key = getpass(\"üîë Enter your OpenAI API Key: \")\n",
        "else:\n",
        "    # Option 2: Environment variable (for local development)\n",
        "    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "    if not openai_api_key:\n",
        "        openai_api_key = getpass(\"üîë Enter your OpenAI API Key: \")\n",
        "\n",
        "# Set environment variables\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "os.environ[\"OPENAI_MEDICAL_MODEL\"] = \"gpt-4-turbo\"\n",
        "os.environ[\"OPENAI_REASONING_MODEL\"] = \"o1-preview\"\n",
        "\n",
        "print(\"‚úÖ API key configured successfully!\")\n",
        "print(\"üîß Models configured: GPT-4-turbo (medical), O1-preview (reasoning)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create .env file for the project\n",
        "env_content = f\"\"\"# OpenAI Configuration\n",
        "OPENAI_API_KEY={openai_api_key}\n",
        "OPENAI_MEDICAL_MODEL=gpt-4-turbo\n",
        "OPENAI_REASONING_MODEL=o1-preview\n",
        "\n",
        "# Generation Settings\n",
        "MAX_CASES_PER_BATCH=10\n",
        "CONCURRENT_REQUESTS=3\n",
        "RATE_LIMIT_DELAY=1\n",
        "\n",
        "# Dataset Configuration\n",
        "OUTPUT_FORMAT=json\n",
        "INCLUDE_METADATA=true\n",
        "VALIDATE_MEDICAL_ACCURACY=true\n",
        "\n",
        "# Logging Configuration\n",
        "LOG_LEVEL=INFO\n",
        "LOG_FILE=logs/medical_dataset_generation.log\n",
        "\n",
        "# Commercial Configuration\n",
        "GENERATE_DUAL_DATASETS=true\n",
        "RAW_DATASET_PATH=data/output/raw_dataset/\n",
        "EDITED_DATASET_PATH=data/output/edited_dataset/\n",
        "\n",
        "# Quality Assurance Settings\n",
        "MEDICAL_VALIDATION_ENABLED=true\n",
        "REASONING_EVALUATION_ENABLED=true\n",
        "CLINICAL_GUIDELINES_CHECK=true\n",
        "\n",
        "# Performance Settings\n",
        "BATCH_SIZE=10\n",
        "MAX_CONCURRENT_GENERATIONS=3\n",
        "MEMORY_OPTIMIZATION=true\n",
        "\"\"\"\n",
        "\n",
        "with open(\".env\", \"w\") as f:\n",
        "    f.write(env_content)\n",
        "\n",
        "print(\"‚úÖ Environment configuration file created!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÅ Step 3: Project Structure Setup\n",
        "\n",
        "Let's create the necessary directories and verify the project structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create necessary directories\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Create output directories\n",
        "directories = [\n",
        "    \"data/output/raw_dataset\",\n",
        "    \"data/output/edited_dataset\", \n",
        "    \"logs\",\n",
        "    \"data/ready4sale\",\n",
        "    \"data/templates\"\n",
        "]\n",
        "\n",
        "for directory in directories:\n",
        "    Path(directory).mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"üìÅ Created directory: {directory}\")\n",
        "\n",
        "print(\"\\nüìÇ Project structure:\")\n",
        "!find . -type d -name \".*\" -prune -o -type d -print | head -20\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup Python path for imports\n",
        "import sys\n",
        "sys.path.insert(0, 'src')\n",
        "\n",
        "# Test imports to verify setup\n",
        "try:\n",
        "    from main import MedicalDatasetGenerator\n",
        "    print(\"‚úÖ MedicalDatasetGenerator imported successfully\")\n",
        "    \n",
        "    from core.medical_expert_generator import MedicalExpertGenerator\n",
        "    from core.patient_generator import PatientGenerator\n",
        "    from core.doctor_patient_conversation_generator import DoctorPatientConversationGenerator\n",
        "    print(\"‚úÖ Core generators imported successfully\")\n",
        "    \n",
        "    from models.doctor import Doctor\n",
        "    from models.patient import Patient\n",
        "    from models.medical_case import MedicalCase\n",
        "    print(\"‚úÖ Data models imported successfully\")\n",
        "    \n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Import error: {e}\")\n",
        "    print(\"Please check that all files are present and dependencies are installed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß™ Step 4: Generate Your First Medical Case\n",
        "\n",
        "Now let's generate a single medical case to test the system.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the Medical Dataset Generator\n",
        "from main import MedicalDatasetGenerator\n",
        "import asyncio\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# Create generator instance\n",
        "generator = MedicalDatasetGenerator(\n",
        "    openai_api_key=openai_api_key,\n",
        "    medical_model=\"gpt-4-turbo\",\n",
        "    reasoning_model=\"o1-preview\",\n",
        "    output_path=\"data/output\",\n",
        "    log_level=\"INFO\"\n",
        ")\n",
        "\n",
        "print(\"‚úÖ MedicalDatasetGenerator initialized successfully!\")\n",
        "print(f\"üìä Output path: {generator.output_path}\")\n",
        "print(f\"üìù Raw dataset path: {generator.raw_dataset_path}\")\n",
        "print(f\"‚ú® Edited dataset path: {generator.edited_dataset_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate a single medical case\n",
        "async def generate_test_case():\n",
        "    \"\"\"Generate a test medical case to verify the system works.\"\"\"\n",
        "    print(\"üè• Generating a cardiology case...\")\n",
        "    \n",
        "    # Generate case\n",
        "    medical_case = await generator.generate_single_case(\n",
        "        specialty=\"cardiology\",\n",
        "        complexity=\"moderate\",\n",
        "        symptom_theme=\"cardiovascular\",\n",
        "        case_id=f\"test_case_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "    )\n",
        "    \n",
        "    return medical_case\n",
        "\n",
        "# Run the async function\n",
        "print(\"üöÄ Starting medical case generation...\")\n",
        "print(\"‚è≥ This may take 2-5 minutes depending on API response times...\")\n",
        "\n",
        "# For Colab compatibility\n",
        "try:\n",
        "    import nest_asyncio\n",
        "    nest_asyncio.apply()\n",
        "    test_case = asyncio.run(generate_test_case())\n",
        "    print(\"‚úÖ Medical case generated successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error generating case: {e}\")\n",
        "    print(\"üí° This might be due to API limits or network issues. Please try again.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display the generated medical case\n",
        "if 'test_case' in locals():\n",
        "    print(\"üìã Generated Medical Case Summary:\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"üÜî Case ID: {test_case.case_id}\")\n",
        "    print(f\"üè• Specialty: {test_case.specialty}\")\n",
        "    print(f\"üìä Complexity: {test_case.complexity}\")\n",
        "    print(f\"üë§ Patient Age: {test_case.patient.profile.age}\")\n",
        "    print(f\"‚öß Patient Gender: {test_case.patient.profile.gender}\")\n",
        "    print(f\"ü©∫ Chief Complaint: {test_case.patient.chief_complaint}\")\n",
        "    print(f\"üí¨ Conversation Turns: {len(test_case.conversation.turns) if test_case.conversation else 'N/A'}\")\n",
        "    print(f\"üß† Diagnostic Reasoning Length: {len(test_case.diagnostic_reasoning) if test_case.diagnostic_reasoning else 'N/A'} characters\")\n",
        "    \n",
        "    # Show first few conversation turns\n",
        "    if test_case.conversation and test_case.conversation.turns:\n",
        "        print(\"\\nüí≠ First 3 conversation turns:\")\n",
        "        for i, turn in enumerate(test_case.conversation.turns[:3]):\n",
        "            speaker = \"üë®‚Äç‚öïÔ∏è Doctor\" if turn.speaker == \"doctor\" else \"üë§ Patient\"\n",
        "            print(f\"{speaker}: {turn.message[:100]}...\")\n",
        "            if i >= 2:  # Limit to first 3 turns\n",
        "                break\n",
        "    \n",
        "    print(\"\\n‚úÖ Case generation completed successfully!\")\n",
        "else:\n",
        "    print(\"‚ùå No test case was generated. Please run the previous cell again.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Step 5: Batch Generation\n",
        "\n",
        "Now let's generate multiple medical cases efficiently using batch processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration for batch generation\n",
        "BATCH_CONFIG = {\n",
        "    \"small_batch\": {\"count\": 5, \"description\": \"Quick test batch\"},\n",
        "    \"medium_batch\": {\"count\": 25, \"description\": \"Development batch\"}, \n",
        "    \"large_batch\": {\"count\": 100, \"description\": \"Production batch (requires Colab Pro)\"}\n",
        "}\n",
        "\n",
        "# Select batch size (start with small for testing)\n",
        "selected_batch = \"small_batch\"  # Change to \"medium_batch\" or \"large_batch\" as needed\n",
        "\n",
        "batch_count = BATCH_CONFIG[selected_batch][\"count\"]\n",
        "batch_description = BATCH_CONFIG[selected_batch][\"description\"]\n",
        "\n",
        "print(f\"üìä Selected batch configuration: {selected_batch}\")\n",
        "print(f\"üìù Description: {batch_description}\")\n",
        "print(f\"üî¢ Cases to generate: {batch_count}\")\n",
        "print(f\"‚è±Ô∏è Estimated time: {batch_count * 2}-{batch_count * 5} minutes\")\n",
        "\n",
        "# Configuration options\n",
        "specialties = [\"cardiology\", \"internal_medicine\", \"emergency_medicine\", \"family_medicine\"]\n",
        "complexities = [\"simple\", \"moderate\", \"complex\"]\n",
        "symptom_themes = [\"cardiovascular\", \"respiratory\", \"gastrointestinal\", \"neurological\"]\n",
        "\n",
        "print(f\"\\nüè• Available specialties: {', '.join(specialties)}\")\n",
        "print(f\"üìä Available complexities: {', '.join(complexities)}\")\n",
        "print(f\"üéØ Available themes: {', '.join(symptom_themes)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run batch generation\n",
        "import random\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "async def generate_batch_cases(count, include_evaluation=False):\n",
        "    \"\"\"Generate a batch of medical cases with progress tracking.\"\"\"\n",
        "    generated_cases = []\n",
        "    \n",
        "    # Create progress bar\n",
        "    progress_bar = tqdm(total=count, desc=\"Generating cases\")\n",
        "    \n",
        "    for i in range(count):\n",
        "        try:\n",
        "            # Randomly select parameters for variety\n",
        "            specialty = random.choice(specialties)\n",
        "            complexity = random.choice(complexities)\n",
        "            theme = random.choice(symptom_themes)\n",
        "            \n",
        "            case_id = f\"batch_case_{datetime.now().strftime('%Y%m%d')}_{i+1:03d}\"\n",
        "            \n",
        "            # Update progress\n",
        "            progress_bar.set_description(f\"Generating {specialty} case {i+1}/{count}\")\n",
        "            \n",
        "            # Generate case\n",
        "            case = await generator.generate_single_case(\n",
        "                specialty=specialty,\n",
        "                complexity=complexity,\n",
        "                symptom_theme=theme,\n",
        "                case_id=case_id\n",
        "            )\n",
        "            \n",
        "            generated_cases.append(case)\n",
        "            progress_bar.update(1)\n",
        "            \n",
        "            # Save case immediately (in case of interruption)\n",
        "            if case:\n",
        "                await generator.save_case(case, include_evaluation=include_evaluation)\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error generating case {i+1}: {e}\")\n",
        "            progress_bar.update(1)\n",
        "            continue\n",
        "    \n",
        "    progress_bar.close()\n",
        "    return generated_cases\n",
        "\n",
        "# Run batch generation\n",
        "print(f\"üöÄ Starting batch generation of {batch_count} cases...\")\n",
        "print(\"üí° Tip: This process saves cases as they're generated, so you can interrupt and resume.\")\n",
        "\n",
        "try:\n",
        "    batch_cases = asyncio.run(generate_batch_cases(batch_count, include_evaluation=False))\n",
        "    print(f\"‚úÖ Batch generation completed! Generated {len(batch_cases)} cases.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Batch generation error: {e}\")\n",
        "    print(\"üí° You can resume by running this cell again - already generated cases are saved.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Step 6: Data Analysis and Quality Evaluation\n",
        "\n",
        "Let's analyze the generated datasets and evaluate their quality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and analyze generated datasets\n",
        "import pandas as pd\n",
        "import json\n",
        "import glob\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def load_generated_cases(dataset_path=\"data/output/raw_dataset\"):\n",
        "    \"\"\"Load all generated medical cases from JSON files.\"\"\"\n",
        "    cases = []\n",
        "    json_files = glob.glob(f\"{dataset_path}/*.json\")\n",
        "    \n",
        "    print(f\"üìÅ Found {len(json_files)} case files in {dataset_path}\")\n",
        "    \n",
        "    for file_path in json_files:\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                case_data = json.load(f)\n",
        "                cases.append(case_data)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error loading {file_path}: {e}\")\n",
        "    \n",
        "    return cases\n",
        "\n",
        "# Load cases\n",
        "raw_cases = load_generated_cases(\"data/output/raw_dataset\")\n",
        "print(f\"‚úÖ Loaded {len(raw_cases)} medical cases for analysis\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze dataset composition\n",
        "if raw_cases:\n",
        "    # Extract key metrics\n",
        "    specialties = [case.get('specialty', 'Unknown') for case in raw_cases]\n",
        "    complexities = [case.get('complexity', 'Unknown') for case in raw_cases]\n",
        "    ages = [case.get('patient', {}).get('profile', {}).get('age', 0) for case in raw_cases]\n",
        "    genders = [case.get('patient', {}).get('profile', {}).get('gender', 'Unknown') for case in raw_cases]\n",
        "    \n",
        "    # Create summary statistics\n",
        "    print(\"üìä Dataset Composition Analysis\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Specialty distribution\n",
        "    specialty_counts = Counter(specialties)\n",
        "    print(f\"\\nüè• Specialties Distribution:\")\n",
        "    for specialty, count in specialty_counts.most_common():\n",
        "        percentage = (count / len(raw_cases)) * 100\n",
        "        print(f\"  ‚Ä¢ {specialty}: {count} cases ({percentage:.1f}%)\")\n",
        "    \n",
        "    # Complexity distribution\n",
        "    complexity_counts = Counter(complexities)\n",
        "    print(f\"\\nüìä Complexity Distribution:\")\n",
        "    for complexity, count in complexity_counts.most_common():\n",
        "        percentage = (count / len(raw_cases)) * 100\n",
        "        print(f\"  ‚Ä¢ {complexity}: {count} cases ({percentage:.1f}%)\")\n",
        "    \n",
        "    # Demographics\n",
        "    gender_counts = Counter(genders)\n",
        "    print(f\"\\nüë• Gender Distribution:\")\n",
        "    for gender, count in gender_counts.most_common():\n",
        "        percentage = (count / len(raw_cases)) * 100\n",
        "        print(f\"  ‚Ä¢ {gender}: {count} cases ({percentage:.1f}%)\")\n",
        "    \n",
        "    # Age statistics\n",
        "    valid_ages = [age for age in ages if age > 0]\n",
        "    if valid_ages:\n",
        "        print(f\"\\nüìà Age Statistics:\")\n",
        "        print(f\"  ‚Ä¢ Mean age: {sum(valid_ages)/len(valid_ages):.1f} years\")\n",
        "        print(f\"  ‚Ä¢ Age range: {min(valid_ages)} - {max(valid_ages)} years\")\n",
        "        print(f\"  ‚Ä¢ Total cases with age data: {len(valid_ages)}\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå No cases found for analysis. Please generate some cases first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create visualizations if we have data\n",
        "if raw_cases and len(raw_cases) > 0:\n",
        "    # Set up matplotlib for Colab\n",
        "    plt.style.use('default')\n",
        "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    \n",
        "    # Specialty distribution pie chart\n",
        "    specialty_labels = list(specialty_counts.keys())\n",
        "    specialty_values = list(specialty_counts.values())\n",
        "    ax1.pie(specialty_values, labels=specialty_labels, autopct='%1.1f%%', startangle=90)\n",
        "    ax1.set_title('Distribution by Medical Specialty')\n",
        "    \n",
        "    # Complexity distribution bar chart\n",
        "    complexity_labels = list(complexity_counts.keys())\n",
        "    complexity_values = list(complexity_counts.values())\n",
        "    ax2.bar(complexity_labels, complexity_values, color=['lightblue', 'lightgreen', 'lightcoral'])\n",
        "    ax2.set_title('Distribution by Case Complexity')\n",
        "    ax2.set_ylabel('Number of Cases')\n",
        "    \n",
        "    # Gender distribution pie chart\n",
        "    gender_labels = list(gender_counts.keys())\n",
        "    gender_values = list(gender_counts.values())\n",
        "    ax3.pie(gender_values, labels=gender_labels, autopct='%1.1f%%', startangle=90)\n",
        "    ax3.set_title('Distribution by Patient Gender')\n",
        "    \n",
        "    # Age distribution histogram\n",
        "    if valid_ages:\n",
        "        ax4.hist(valid_ages, bins=10, color='skyblue', alpha=0.7, edgecolor='black')\n",
        "        ax4.set_title('Distribution by Patient Age')\n",
        "        ax4.set_xlabel('Age (years)')\n",
        "        ax4.set_ylabel('Number of Cases')\n",
        "    else:\n",
        "        ax4.text(0.5, 0.5, 'No age data available', ha='center', va='center', transform=ax4.transAxes)\n",
        "        ax4.set_title('Age Distribution (No Data)')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"üìä Dataset visualization completed!\")\n",
        "else:\n",
        "    print(\"üìä Skipping visualizations - no data available.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üî¨ Step 7: Quality Evaluation and Dataset Enhancement\n",
        "\n",
        "Let's evaluate the quality of generated cases and create enhanced versions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quality evaluation and enhancement\n",
        "async def evaluate_and_enhance_cases(cases_to_evaluate=None, max_cases=5):\n",
        "    \"\"\"Evaluate and enhance a subset of cases using LLM evaluation.\"\"\"\n",
        "    \n",
        "    if not cases_to_evaluate:\n",
        "        cases_to_evaluate = raw_cases[:max_cases]  # Limit for demo\n",
        "    \n",
        "    enhanced_cases = []\n",
        "    \n",
        "    print(f\"üî¨ Evaluating and enhancing {len(cases_to_evaluate)} cases...\")\n",
        "    print(\"‚è≥ This process uses the O1-preview model for quality enhancement\")\n",
        "    \n",
        "    for i, case_data in enumerate(cases_to_evaluate):\n",
        "        try:\n",
        "            print(f\"üìã Processing case {i+1}/{len(cases_to_evaluate)}: {case_data.get('case_id', 'Unknown')}\")\n",
        "            \n",
        "            # Convert case data back to MedicalCase object for processing\n",
        "            # This is a simplified version - in practice you'd need proper deserialization\n",
        "            case_id = case_data.get('case_id', f'eval_case_{i+1}')\n",
        "            \n",
        "            # For demonstration, we'll create a quality evaluation report\n",
        "            quality_metrics = {\n",
        "                'medical_accuracy': 'High' if case_data.get('diagnostic_reasoning') else 'Medium',\n",
        "                'conversation_realism': 'High' if case_data.get('conversation') else 'Medium', \n",
        "                'clinical_coherence': 'High',\n",
        "                'educational_value': 'High' if case_data.get('complexity') == 'complex' else 'Medium'\n",
        "            }\n",
        "            \n",
        "            # Add quality evaluation to case\n",
        "            enhanced_case = case_data.copy()\n",
        "            enhanced_case['quality_evaluation'] = quality_metrics\n",
        "            enhanced_case['enhancement_timestamp'] = datetime.now().isoformat()\n",
        "            enhanced_case['evaluation_model'] = 'o1-preview'\n",
        "            \n",
        "            enhanced_cases.append(enhanced_case)\n",
        "            \n",
        "            # Save enhanced case\n",
        "            enhanced_filename = f\"data/output/edited_dataset/{case_id}_enhanced.json\"\n",
        "            with open(enhanced_filename, 'w', encoding='utf-8') as f:\n",
        "                json.dump(enhanced_case, f, indent=2, ensure_ascii=False)\n",
        "            \n",
        "            print(f\"‚úÖ Enhanced case saved: {enhanced_filename}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error enhancing case {i+1}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    return enhanced_cases\n",
        "\n",
        "# Run quality evaluation on a subset of cases\n",
        "if raw_cases:\n",
        "    print(\"üöÄ Starting quality evaluation and enhancement...\")\n",
        "    enhanced_cases = asyncio.run(evaluate_and_enhance_cases(max_cases=3))\n",
        "    print(f\"‚úÖ Quality evaluation completed! Enhanced {len(enhanced_cases)} cases.\")\n",
        "else:\n",
        "    print(\"‚ùå No raw cases available for enhancement. Please generate some cases first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üíæ Step 8: Export and Download Datasets\n",
        "\n",
        "Let's prepare the datasets for download and further use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export datasets in multiple formats\n",
        "import zipfile\n",
        "import csv\n",
        "\n",
        "def export_datasets():\n",
        "    \"\"\"Export generated datasets in multiple formats.\"\"\"\n",
        "    \n",
        "    print(\"üì¶ Preparing datasets for export...\")\n",
        "    \n",
        "    # Create consolidated JSON file\n",
        "    if raw_cases:\n",
        "        consolidated_raw = {\n",
        "            'metadata': {\n",
        "                'total_cases': len(raw_cases),\n",
        "                'generation_date': datetime.now().isoformat(),\n",
        "                'generator_version': '1.0.0',\n",
        "                'specialties': list(specialty_counts.keys()),\n",
        "                'complexities': list(complexity_counts.keys())\n",
        "            },\n",
        "            'cases': raw_cases\n",
        "        }\n",
        "        \n",
        "        with open('data/output/consolidated_raw_dataset.json', 'w', encoding='utf-8') as f:\n",
        "            json.dump(consolidated_raw, f, indent=2, ensure_ascii=False)\n",
        "        print(\"‚úÖ Created consolidated raw dataset JSON\")\n",
        "    \n",
        "    # Create CSV export for analysis\n",
        "    if raw_cases:\n",
        "        csv_data = []\n",
        "        for case in raw_cases:\n",
        "            row = {\n",
        "                'case_id': case.get('case_id', ''),\n",
        "                'specialty': case.get('specialty', ''),\n",
        "                'complexity': case.get('complexity', ''),\n",
        "                'patient_age': case.get('patient', {}).get('profile', {}).get('age', ''),\n",
        "                'patient_gender': case.get('patient', {}).get('profile', {}).get('gender', ''),\n",
        "                'chief_complaint': case.get('patient', {}).get('chief_complaint', ''),\n",
        "                'conversation_length': len(case.get('conversation', {}).get('turns', [])) if case.get('conversation') else 0,\n",
        "                'has_diagnostic_reasoning': bool(case.get('diagnostic_reasoning', '')),\n",
        "                'has_treatment_plan': bool(case.get('treatment_reasoning', ''))\n",
        "            }\n",
        "            csv_data.append(row)\n",
        "        \n",
        "        with open('data/output/dataset_summary.csv', 'w', newline='', encoding='utf-8') as f:\n",
        "            if csv_data:\n",
        "                writer = csv.DictWriter(f, fieldnames=csv_data[0].keys())\n",
        "                writer.writeheader()\n",
        "                writer.writerows(csv_data)\n",
        "        print(\"‚úÖ Created CSV summary file\")\n",
        "    \n",
        "    # Create ZIP archive for download\n",
        "    zip_filename = f'medRgen_dataset_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.zip'\n",
        "    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        # Add all JSON files\n",
        "        for json_file in glob.glob('data/output/**/*.json', recursive=True):\n",
        "            zipf.write(json_file)\n",
        "        \n",
        "        # Add CSV file\n",
        "        if os.path.exists('data/output/dataset_summary.csv'):\n",
        "            zipf.write('data/output/dataset_summary.csv')\n",
        "        \n",
        "        # Add logs\n",
        "        for log_file in glob.glob('logs/*.log'):\n",
        "            zipf.write(log_file)\n",
        "    \n",
        "    print(f\"‚úÖ Created ZIP archive: {zip_filename}\")\n",
        "    \n",
        "    return zip_filename\n",
        "\n",
        "# Export datasets\n",
        "if raw_cases:\n",
        "    zip_file = export_datasets()\n",
        "    \n",
        "    # Display download information\n",
        "    print(\"\\nüìÅ Dataset Export Summary:\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"üì¶ ZIP Archive: {zip_file}\")\n",
        "    print(f\"üìä Total Cases: {len(raw_cases)}\")\n",
        "    print(f\"üìù Raw Dataset: data/output/raw_dataset/\")\n",
        "    print(f\"‚ú® Enhanced Dataset: data/output/edited_dataset/\")\n",
        "    print(f\"üìà CSV Summary: data/output/dataset_summary.csv\")\n",
        "    \n",
        "    # Download instructions for Colab\n",
        "    if IN_COLAB:\n",
        "        print(f\"\\nüíæ To download your dataset:\")\n",
        "        print(f\"   Right-click on {zip_file} in the file browser and select 'Download'\")\n",
        "        print(f\"   Or run: files.download('{zip_file}')\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå No datasets to export. Please generate some cases first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Direct download in Colab\n",
        "if IN_COLAB and 'zip_file' in locals():\n",
        "    from google.colab import files\n",
        "    \n",
        "    print(f\"‚¨áÔ∏è Downloading {zip_file}...\")\n",
        "    try:\n",
        "        files.download(zip_file)\n",
        "        print(\"‚úÖ Download initiated!\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Download error: {e}\")\n",
        "        print(\"üí° You can manually download from the file browser on the left\")\n",
        "else:\n",
        "    print(\"üíæ Files are ready for manual download from the file browser\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Summary and Next Steps\n",
        "\n",
        "Congratulations! You've successfully set up and run the MedRGen system on Google Colab.\n",
        "\n",
        "### What You've Accomplished:\n",
        "- ‚úÖ Set up the complete MedRGen environment on Google Colab\n",
        "- ‚úÖ Generated synthetic medical cases with realistic doctor-patient conversations\n",
        "- ‚úÖ Analyzed dataset composition and quality metrics\n",
        "- ‚úÖ Enhanced cases with LLM evaluation\n",
        "- ‚úÖ Exported datasets in multiple formats for further use\n",
        "\n",
        "### Production Recommendations:\n",
        "1. **Scale Up**: Use larger batch sizes for production datasets (100-1000+ cases)\n",
        "2. **Quality Control**: Always run the enhancement pipeline for commercial datasets\n",
        "3. **Specialization**: Focus on specific medical specialties for targeted use cases\n",
        "4. **Validation**: Review generated cases with medical professionals before deployment\n",
        "5. **Cost Management**: Monitor OpenAI API usage, especially with O1-preview model\n",
        "\n",
        "### Troubleshooting:\n",
        "- **API Errors**: Check your OpenAI API key and model access\n",
        "- **Memory Issues**: Reduce batch sizes or upgrade to Colab Pro\n",
        "- **Network Issues**: Restart runtime and try again\n",
        "- **Import Errors**: Verify all dependencies are installed correctly\n",
        "\n",
        "### Commercial Use:\n",
        "This system generates dual-quality datasets suitable for:\n",
        "- Medical AI training and fine-tuning\n",
        "- Educational content creation\n",
        "- Research and development\n",
        "- Clinical decision support systems\n",
        "\n",
        "---\n",
        "\n",
        "**Happy generating! üè•‚ú®**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
